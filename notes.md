# Notes on the Research Process

After creating the initial naive implementation, 
I had to identify shortcomings and techniques to 
improve the Mad Lib generator. 

## Fine-Grained Parts of Speech

In the naive implementation, I use the basic, 
coarse-grained part-of-speech tagging provided in 
SpaCy. This library also includes a more fine-
grained POS tagger based on the Penn Treebank 
Project dataset. 

The Penn Treebank operated from 1989 to 1996, 
producing more than 7 million words of English 
POS-tagged text across a wide range of material.
It categorises words into 36 detailed POS tags, 
capturing basic inflectional properties of verbs 
and nouns. These tags are clearly defined in 
this document:

Beatrice Santorini. 1990. _Part-of-speech tagging 
guidelines for the penn treebank project._ 
Technical report MS-CIS-90-47. University of 
Pennsylvania.

I analysed this document to identify possibilities 
for improvement using this more specific 
categorization. Even when the category is 
essentially the same as the simpler POS tagger, 
these tags will be more refined because of sub-
categorization. For example, using the adjective 
tag from this dataset will exclude comparative 
and superlative adjectives. 

### Adjectives

The Penn Treebank tags differentiate adjectives
(including ordinals) `JJ` from comparative `JJR` 
and superlative `JJS` adjectives. 

The same is true for adverbs (`RB`, `RBR`, `RBS`).

### Numbers

The Penn Treebank tags pull out cardinal numbers 
into their own tag, `CD`. This is a tag in the 
original Mad Libs. 

### Interjections

Also known as exclamations, interjections are 
tagged with `UH`. This is a tag in the original 
Mad Libs.

### Nouns

Nouns are split into singular/mass nouns `NN` or 
plural nouns `NNS`. However, there is no distinc-
tion between singular and mass nouns. 

Proper nouns are tagged as `NP` for singular and 
`NPS` for plural. 

### Pronouns

Most pronouns are grouped under `PP`, including 
when inflected for nominative or accusative case 
and reflexive pronouns. Possessive pronouns
(genitive case) are grouped under `PP$`. 

### Verbs

The base form of verbs (lexeme entry) is tagged 
as `VB`. This includes imperatives, infinitives, 
and subjunctives. 

Past tense verbs are tagged with `VBD`. Gerunds/
present participles are tagged with `VBG`. Past 
participles are tagged with `VBN`. 

Present tense verbs are divided into 3rd person 
singular `VBZ` and everything else `VBP`. 

In the original Mad Libs, verbs are either the 
base form ("verb") or the gerund form ("verb 
ending with 'ing'"). 

### Improvements

Using these tags, I updated the text processor to 
include adjectives, adverbs, numbers, exclamations, 
singular/plural nouns and proper nouns, and verbs, 
both the base form and the gerund form. 

## Proper Noun Classification

With these more fine-grained parts of speech, 
we are able to pick out proper nouns in contrast 
to regular objects. However, there are meaning-
ful subdivisions within proper nouns, from 
people to countries to organizations. 

Named Entity Recognition (NER) is another 
preprocessing functionality built into the 
SpaCy pipeline. The EntityRecognizer picks out 
[various types of named entities](https://spacy.io/models/en#en_core_web_sm-labels). 
This includes countries or cities `GPE`, 
organizations `ORG`, and people `PERSON`. 

These tags hold promise as replaceable words in 
an autogenerated Mad Lib. However, many of these 
entities correspond to a series of tokens rather 
than just one; a person is often referred to 
with a first name and a last name. 

Therefore, the first step in incorporating these 
named entities was to retokenize them, so that 
each entity corresponds to a single unit in the 
list of tokens. To do this, I used the SpaCy 
retokenizer, which merges spans of tokens into 
one. 

Next, I updated the blank-creation process to 
include the three named entities mentioned above.

## Pronoun Gender Mismatch

Now that named entities are being subcategorized 
and blanked, people's names are being replaced. 
However, this poses an issue for other mentions 
of a person, since English inflects pronouns 
based on gender. Therefore, a sentence like 
"Erik helped himself to a slice of cake" 
might end up being "Sophie helped himself to 
a shed of cake". 

If Sophie uses she/her pronouns, this mismatch
in pronoun gender 
could be considered ungrammatical or even 
distressing. If Erik is being replaced with a 
different name, it would be ideal to specify 
the pronouns that will be associated with 
a replacement name; this is my next goal. 

To make this possible, two functionalities are 
needed. First, we need to match up names with 
their associated pronouns. Then, we need to 
classify pronouns and use them to update the 
fill-in-the-blank prompt. 

### Coreference Resolution

Coreference resolution is an NLP preprocessing 
task that seeks to identify all mentions that 
refer to the same entity in the discourse world. 
So, any time a set of multiple words reference 
the same person or thing, coref resolution 
tries to identify this set. 

[Coreferee](https://github.com/richardpaulhudson/coreferee) 
is an open source library that interfaces with 
SpaCy to provide coreference resolution. It uses 
hardcoded rules to help resolve coreferences, 
including lists of words that are inherently 
male/female, or inherently animate/inanimate, 
which impact possible pronoun associations. 
It also uses machine learning and neural 
networks to identify subtle patterns in how 
an entity tends to be referenced multiple times.

Using Coreferee to identify coreferences, I 
associate each person's name with a list of 
pronouns it matches. 

### Feature Identification

SpaCy comes with a Morphologizer that assigns 
morphological features to tokens according to 
the [Universal Dependencies FEATS format](https://universaldependencies.org/format.html#morphological-annotation).

For each person's name, I extract the gender 
features from every token in the list of 
coreferences. If these coreferences are 
uniformly masculine or uniformly feminine, 
I update the prompt to specify these 
preferred pronouns. 

Unfortunately, the Coreferee and Morphologizer 
are poor at identifying the use of singular they 
or other pronouns. This probably stems from both 
the conflation between singular and plural they
(in contrast to the unambiguous he/she), from 
a relative dearth of singular they usages in 
training data, and from biases in the programmers 
who built these libraries. This means that the 
generator cannot identify when a person uses 
they/them or other pronouns; when it is not 
clearly he/him or she/her, no pronouns are 
indicated in the prompt. 
