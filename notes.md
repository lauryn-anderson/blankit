# Notes on the Research Process

After creating the initial naive implementation, 
I had to identify shortcomings and techniques to 
improve the Mad Lib generator. 

## Fine-Grained Parts of Speech

In the naive implementation, I use the basic, 
coarse-grained part-of-speech tagging provided in 
SpaCy. This library also includes a more fine-
grained POS tagger based on the Penn Treebank 
Project dataset. 

The Penn Treebank operated from 1989 to 1996, 
producing more than 7 million words of English 
POS-tagged text across a wide range of material.
It categorises words into 36 detailed POS tags, 
capturing basic inflectional properties of verbs 
and nouns. These tags are clearly defined in 
this document:

Beatrice Santorini. 1990. _Part-of-speech tagging 
guidelines for the penn treebank project._ 
Technical report MS-CIS-90-47. University of 
Pennsylvania.

I analysed this document to identify possibilities 
for improvement using this more specific 
categorization. Even when the category is 
essentially the same as the simpler POS tagger, 
these tags will be more refined because of sub-
categorization. For example, using the adjective 
tag from this dataset will exclude comparative 
and superlative adjectives. 

### Adjectives

The Penn Treebank tags differentiate adjectives
(including ordinals) `JJ` from comparative `JJR` 
and superlative `JJS` adjectives. 

The same is true for adverbs (`RB`, `RBR`, `RBS`).

### Numbers

The Penn Treebank tags pull out cardinal numbers 
into their own tag, `CD`. This is a tag in the 
original Mad Libs. 

### Interjections

Also known as exclamations, interjections are 
tagged with `UH`. This is a tag in the original 
Mad Libs.

### Nouns

Nouns are split into singular/mass nouns `NN` or 
plural nouns `NNS`. However, there is no distinc-
tion between singular and mass nouns. 

Proper nouns are tagged as `NP` for singular and 
`NPS` for plural. 

### Pronouns

Most pronouns are grouped under `PP`, including 
when inflected for nominative or accusative case 
and reflexive pronouns. Possessive pronouns
(genitive case) are grouped under `PP$`. 

### Verbs

The base form of verbs (lexeme entry) is tagged 
as `VB`. This includes imperatives, infinitives, 
and subjunctives. 

Past tense verbs are tagged with `VBD`. Gerunds/
present participles are tagged with `VBG`. Past 
participles are tagged with `VBN`. 

Present tense verbs are divided into 3rd person 
singular `VBZ` and everything else `VBP`. 

In the original Mad Libs, verbs are either the 
base form ("verb") or the gerund form ("verb 
ending with 'ing'"). 

### Improvements

Using these tags, I updated the text processor to 
include adjectives, adverbs, numbers, exclamations, 
singular/plural nouns and proper nouns, and verbs, 
both the base form and the gerund form. 

## Proper Noun Classification

With these more fine-grained parts of speech, 
we are able to pick out proper nouns in contrast 
to regular objects. However, there are meaning-
ful subdivisions within proper nouns, from 
people to countries to organizations. 

Named Entity Recognition (NER) is another 
preprocessing functionality built into the 
SpaCy pipeline. The EntityRecognizer picks out 
[various types of named entities](https://spacy.io/models/en#en_core_web_sm-labels). 
This includes countries or cities `GPE`, 
organizations `ORG`, and people `PERSON`. 

These tags hold promise as replaceable words in 
an autogenerated Mad Lib. However, many of these 
entities correspond to a series of tokens rather 
than just one; a person is often referred to 
with a first name and a last name. 

Therefore, the first step in incorporating these 
named entities was to retokenize them, so that 
each entity corresponds to a single unit in the 
list of tokens. To do this, I used the SpaCy 
retokenizer, which merges spans of tokens into 
one. 

Next, I updated the blank-creation process to 
include the three named entities mentioned above.

## Preferred Pronoun Identification

Now that named entities are being subcategorized 
and blanked, people's names are being replaced. 
However, this poses an issue for other mentions 
of a person, since English inflects pronouns 
based on gender. Therefore, a sentence like 
"Erik helped himself to a slice of cake" 
might end up being "Sophie helped himself to 
a shed of cake". 

If Sophie uses she/her pronouns, this mismatch
in pronoun gender 
could be considered ungrammatical or even 
distressing. If Erik is being replaced with a 
different name, it would be ideal to specify 
the pronouns that will be associated with 
a replacement name; this is my next goal. 

To make this possible, two functionalities are 
needed. First, we need to match up names with 
their associated pronouns. Then, we need to 
classify pronouns and use them to update the 
fill-in-the-blank prompt. 

### Coreference Resolution

Coreference resolution is an NLP preprocessing 
task that seeks to identify all mentions that 
refer to the same entity in the discourse world. 
So, any time a set of multiple words reference 
the same person or thing, coref resolution 
tries to identify this set. 

[Coreferee](https://github.com/richardpaulhudson/coreferee) 
is an open source library that interfaces with 
SpaCy to provide coreference resolution. It uses 
hardcoded rules to help resolve coreferences, 
including lists of words that are inherently 
male/female, or inherently animate/inanimate, 
which impact possible pronoun associations. 
It also uses machine learning and neural 
networks to identify subtle patterns in how 
an entity tends to be referenced multiple times.

Using Coreferee to identify coreferences, I 
associate each person's name with a list of 
pronouns it matches. 

### Feature Identification

SpaCy comes with a Morphologizer that assigns 
morphological features to tokens according to 
the [Universal Dependencies FEATS format](https://universaldependencies.org/format.html#morphological-annotation).

For each person's name, I extract the gender 
features from every token in the list of 
coreferences. If these coreferences are 
uniformly masculine or uniformly feminine, 
I update the prompt to specify these 
preferred pronouns. 

Unfortunately, the Coreferee and Morphologizer 
are poor at identifying the use of singular they 
or other pronouns. This probably stems from both 
the conflation between singular and plural they
(in contrast to the unambiguous he/she), from 
a relative dearth of singular they usages in 
training data, and from biases in the programmers 
who built these libraries. This means that the 
generator cannot identify when a person uses 
they/them or other pronouns; when it is not 
clearly he/him or she/her, no pronouns are 
indicated in the prompt. 

## Adverb Subcategorization

Adverbs are tricky. A wide range of words are 
all classified as adverbs. This includes:

- verb (phrase) modifiers
  - He _frantically_ ran past the door
  - She _also_ ran past the door
- adjective modifiers
  - This is _overwhelmingly_ delicious
  - We should be _more_ inclusive
- adverb modifiers
  - She spoke _so_ quietly
  - This is _really_ not a good idea
- determiner modifiers
  - _Almost_ no plants are dangerous
- preposition phrase modifiers
  - We drove _nearly_ until midnight

Essentially, adjectives modify nouns, while 
adverbs modify everything else. 
The POS tagger also labels some more 
borderline words as adverbs:

- He does _not_ like to dance

However, most adverbs cannot be exchanged between 
to all of these classes, which could create 
ungrammatical sentences using the generator.

- *He _more_ ran past the door
- *He _very_ ran past the door
- *He _not_ ran past the door

In a Mad Libs context, participants tend to fill 
blanks with adverbs that have more semantic 
content and are often derived from adjectives 
using the suffix _-ly_:

- quick, quickly
- eager, eagerly
- sudden, suddenly
- sad, sadly

These are in contrast to more functional 
adverbs without adjectival equivalents, such as:

- very
- not 
- so 
- almost
- here

Assuming that the replacement words are 
these adjective-based ones, it should be 
grammatical or at least marginal to use them 
to modify some categories:

- verb (phrases)
  - He _boldly_ ran past the door
  - He _suddenly_ ran past the door
- adjectives
  - This is _boldly_ delicious
  - This is _suddenly_ delicious

But not others:

- adverbs
  - *She spoke _boldly_ quietly
- determiners
  - *_Boldly_ no plants are dangerous
- preposition phrases
  - *We drove _boldly_ until midnight 

Therefore, I will use a rule that omits 
_not_ and _n't_ from potential adverb 
blanks, and seek a way to distinguish 
verb/adjective modification from 
adverb/determiner/preposition phrase 
modification. These varied modifier 
relationships are grammatical/syntactic 
dependencies, so I will turn to a syntactic 
preprocessing tool. 
 
### Dependency Parsing

SpaCy provides a Dependency Parser that 
constructs a type of syntax tree from input 
sentences. Given an input sentence, this parser 
generates a series of trees where arcs connect 
each word to the phrase head it depends on, and 
each tree in the series is closer to the optimal 
answer. It uses a combination of a search 
algorithm guided by heuristics, augmented with 
a neural network that lets the algorithm "undo" 
steps in the process that it can only recognize 
later as mistakes. 

Applying this parser, I can get valuable info 
on each adverb in the input text. Specifically, 
every token marked as an adverb is linked to 
the head of its phrase. That way, if it modifies 
a verb, I can look at the token it depends on 
and recognize that it is a verb. If it modifies 
another adverb, I can go to that token and 
identify it as an adverb. 

As long as this dependency parser is effective, 
this provides an effective means to subcategorize 
adverb utterances by the type of category they 
modify. 

## Contractions

Personal testing indicates another shortcoming of 
the current paradigm. The tokenizer is wise enough 
to split contractions into their components.

> I just wanna tell you how I'm feeling 
> 
> Gotta make you understand 
> 
> Never gonna give you up 
> 
> Never gonna let you down

This quote creates tokens like
"got", "ta", "gon", and "na". These do not produce 
grammatical words when selected for blanking; if 
_got_ is replaced with _destroy_, we get 

> destroyta make you understand

Luckily, these constructions can be identified 
by the lack of whitespace between the tokens. 
I updated the generator to find all instances 
of missing whitespace, and to avoid making blanks 
out of the words before and after the missing 
whitespace. 

## Other goals

copular be
capitalization
a/an

We're no strangers to love You know the rules and so do I (do I) A full commitment's what I'm thinking of  You wouldn't get this from any other guy I just wanna tell you how I'm feeling  Gotta make you understand Never gonna give you up Never gonna let you down Never gonna run around and desert you Never gonna make you cry Never gonna say goodbye Never gonna tell a lie and hurt you
